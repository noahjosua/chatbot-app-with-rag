# pip install pipenv
# pipenv install python-dotenv
# pipenv install langchain

# Welcher Document Loader --> CSV
# Welcher Splitter --> https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/
    --> https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/recursive_text_splitter/
    --> Chunk size ermittelt: https://chunkviz.up.railway.app/
        --> stichprobenartig einzelne documents reinkopiert und ausprobiert
            --> chunksize = 650
            --> chunk overlap = 50
    --> nach Splitting angepasst auf chunksize = 700

# Welches Embeddings Model? --> erstmal einfach so: https://python.langchain.com/v0.1/docs/integrations/text_embedding/huggingfacehub/#hugging-face-inference-api mit dem aus dem Beispiel
# Welcher Vector Store? --> erstmal einfach FAISS: https://python.langchain.com/v0.1/docs/integrations/vectorstores/faiss/

# pipenv install langchain-community
# pipenv install faiss-cpu

# Welches LLM? https://python.langchain.com/v0.1/docs/integrations/chat/ --> https://python.langchain.com/v0.1/docs/integrations/chat/huggingface/
    --> HuggingFaceHub

# pipenv install huggingface_hub
# pipenv install transformers

# HuggingFaceEndpoint, weil Hub deprecated
# pipenv install jinja2 (handles chat templates)

# frontend
# pipenv install streamlit
# https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps
# https://docs.streamlit.io/develop/api-reference


# Welches embeddings model? --> https://huggingface.co/sentence-transformers --> https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
    # implementieren
        https://python.langchain.com/v0.1/docs/integrations/text_embedding/sentence_transformers/
        pipenv install sentence-transformers

# Welcher vector store? --> https://myscale.com/blog/faiss-vs-chroma-vector-storage-battle/ --> Chroma
    # implementieren
        https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/
        --> doch FAISS, weil man für Chroma Microsoft C++ Build Tools installieren muss (dependency chroma-hnswlib)
    # TODO einstellen: https://python.langchain.com/v0.1/docs/integrations/vectorstores/faiss/

# Vector store als retriever
      https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/
      https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/
    # TODO einstellen: https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/vectorstore/#maximum-marginal-relevance-retrieval
    # TODO evaluation

# Welches LLM? --> https://huggingface.co/chat/models
    https://huggingface.co/CohereForAI/c4ai-command-r-plus --> braucht pro subscription
    oder
    https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
    # erstmal richtwerte:
                max_new_tokens=512,
                top_k=50,
                temperature=0.3,
                repetition_penalty=1.1

# Prompt Template
    # https://python.langchain.com/v0.1/docs/modules/model_io/prompts/
    # https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/
    # https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval_qa.base.RetrievalQA.html --> langchain.chains.retrieval_qa.base.RetrievalQA (meine aktuelle Chain)
    # https://medium.com/@iryna230520/first-steps-in-langchain-the-ultimate-guide-for-beginners-part-1-2baf5a4e1b81
    # https://nakamasato.medium.com/enhancing-langchains-retrievalqa-for-real-source-links-53713c7d802a
    # https://www.comet.com/site/blog/introduction-to-prompt-templates-in-langchain/
    # https://www.freecodecamp.org/news/beginners-guide-to-langchain/
    # https://nanonets.com/blog/langchain/

# Chains
# https://python.langchain.com/v0.1/docs/modules/chains/
    --> RetrievalQAWithSourcesChain vs. load_qa_with_sources_chain: Ich möchte ersters

--> pip install pipenv
--> pipenv install python-dotenv langchain langchain-community faiss-cpu huggingface_hub transformers jinja2 streamlit sentence-transformers

# metadata bei initialisierung der app modifizieren
# Quellen in Antwort miteinschließen und anständig formatieren

# https://www.restack.io/docs/langchain-knowledge-langchain-chatprompttemplate-guide
# Chat History: https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/
# Chatbots: https://python.langchain.com/v0.1/docs/use_cases/chatbots/
# History mit einbeziehen --> # context (in bezug auf vorherige frage(n)) funktioniert

# resolve Chat Template: jinja2.exceptions.TemplateError: Conversation roles must alternate user/assistant/user/assistant/...

# retriever einstellen
    # data preprocessing
    # dann anhand der query mit k und score_threshold rumspielen (How many seasons does each TV show listed in the dataset have, and what are their respective release years?)

# ich bekomme den richtigen kontext aber das llm kann nicht zählen?
# qa model ausprobieren --> führt aber nicht zum gewünschten ergebnis

# adding contextual compression ausprobieren
# anderes embeddings model ausprobieren Cohere
# anderen vector store ausprobieren DeepLake

# prompt engineering vollkommen underrated


# Evaluation von Retriever begonnen auf evaluations branch, dann beim versuch antworten zu erstellen:
                # anderes Datenset lol. dann muss ich nicht selber ein testdatenset erstellen --> lesson learned --> mehr zeit in die auswahl der datensets stecken :/
                # https://www.kaggle.com/datasets/eyadgk/lamini-taylor-swift?resource=download
# chunk size und overlap nochmal anpassen

# retriever anders eingestellt und system prompt angepasst --> es wurden irrelevante dokumente aus dem store geholt und fragen zu denen es keinerlei info im datensatz gibt wurden beantwortet

            #retriever = vector_store.as_retriever(search_type=constants.SEARCH_TYPE,
                                                 # search_kwargs={constants.K_KEY: 5})

            retriever = vector_store.as_retriever(search_type='similarity_score_threshold',
                                                  search_kwargs={constants.K_KEY: 4, 'score_threshold': 0.5})


TODO
# chat history defekt?
# anzeige anpassen, wenn array leer
# manchmal werden in der antwort die rollen mit angezeigt?! why

# die ganzen deprecated warnings auslöschen


### NICHT LÖSCHEN --> WICHTIG FÜR BERICHT
# Retriever einstellen und evaluieren
    # 1. Evaluationsmetrik: sind retrievte Docs relevant für Frage? mittels Testdatenset, ground of truth
        # anzahl der zurückgegebenen dokumenten ändern, ähnlichkeitsschwelle ändern
        # andere retrievermethode?
        # vektorisierung der dokumente und fragen korrekt durchgeführt?
        # vorverarbeitung der daten anpassen?
        # anderes embeddings model?
        # muss retriever anders eingestellt werden?
        # müssen daten anders aufbereitet werden, bevor sie in DB kommen?
    # 2. Evaluationsmetrik: passt die antwort des LLM zu den retrievten Docs?
        # wie sind prompts geschrieben?
    # 3. Evaluationsmetrik: passt die antwort des LLM zu der userfrage?


# verschiedene datensets reinklatschen
# kleines Modell gegen großes Modell testen, welches hat wo wie performt? --> welche Metriken nutzen?




# Lessons Learned:
    authentifizierung zu hugging face
    dokus richtig lesen erspart arbeit

# Präsentation: 20 min
# Doku: 5 Seiten --> gespeicherte Quellen und Order auf Laptop-Desktop nochmal durchgehen
